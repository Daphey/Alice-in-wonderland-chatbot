{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1ca2948-45f2-4fba-83e1-2080d0fedc8b",
   "metadata": {},
   "source": [
    "## Introduction to Chatbots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d243af-254d-4179-bc50-a939aa9a702b",
   "metadata": {},
   "source": [
    "### A. What are Chatbots?\n",
    "Chatbots are conversational programs that automate interactions. They are artificial intelligence (AI) softwares designed to simulate conversation  with  human users,\n",
    "typically through text or voice.\n",
    "\n",
    "\n",
    "- **Examples**:\n",
    "    - A chatbot on a bank's website that helps with enquiries\n",
    "    - A chatbot on an e-commerce site that tracks orders or provides recommendations\n",
    "    - Virtual assistants like **Siri** and **Alexa**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ee917a-dc93-400a-801d-eee893289634",
   "metadata": {},
   "source": [
    "## B. Difference Between Chatbots and Bots\n",
    "**Chatbots** are a subset of bots. They are specifically designed for conversation, meaning they are programmed to interact using natural language processing (NLP) to simulate human conversations.\n",
    "\n",
    "**Bots**, on the other hand are more general-purpose programs designed to automate tasks. They don't necessarily interact, but they perform specific functions like web scraping, sending reminders or managing social media posts.\n",
    "\n",
    "- **Chatbot**: Focuses on conversation(e.g., answering customer queries).\n",
    "- **Bot**: Focuses on automating repetitive tasks(e.g;, posting scheduled tweets)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fd20fb-7785-4356-abad-2cfa8ec88613",
   "metadata": {},
   "source": [
    "## C. Types of Chatbots\n",
    "**1. Rule-Based Chatbots**:\n",
    "- They follow specific set of instructions or rules. It works by looking for specific keywords or patterns in what you say and then picking the correct response from its list.\n",
    "- The problem is if you ask something it wasn't programmed for, it might get confused or give a response that doesn't make sense.\n",
    "\n",
    "**2. Retrieval-Based Chatbots**:\n",
    "- They are a bit smarter than rule-based ones. Instead of giving a fixed reply, they search through a bunch of pre-written responses and try to find the best one based on what you saaid. It's like going through a library to find the book that most closely answers your question.\n",
    "\n",
    "** - Techniques Used**:\n",
    "   - **Jaccard Similarity**: Imagine you ask a question like, \"What's the weather today?\" The bot checks which of its stored answers have the most words in common with your question. The more words they share, the more likely it is to pick that answer.\n",
    "   - **Cosine Similarity**: This is like comparing two texts using math. It turns your words into numbers and checks how similar they are. If the numbers line up, the bot figures that the answer might be a good fit.\n",
    "   - **Machine Learning Models like `Naive Bayes`**: This is where the bot starts to guess what you're talking about by learning from past examples. If it's trained to answer questions about sports, it'll know that when you ask about \"Football\", it should probably give a sports-related response.\n",
    "\n",
    "\n",
    "**3. Generative Chatbots**:\n",
    "- They are the most advanced chatbots. Instead of pulling from a list of pre-written answers, they create their own responses based on what you said. It's like having a conversation with someone who thinks on the spot and makes up their answers.\n",
    "- However, they need a lot of training to get good at answering questions. They use models like RNNs, LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d219cc3-4b5a-41e2-99c2-d42dfba6df90",
   "metadata": {},
   "source": [
    "### An example illustrating rule-based, retrieval-based, and generative chatbots using a simple customer service scenario related to order tracking\n",
    "\n",
    "##### Scenario\n",
    "The user asks: **\"Where is my order?\"** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd9f2af-2ffe-4d6a-a00b-a31c5aefbf65",
   "metadata": {},
   "source": [
    "**1. Rule-Based Chatbot Example:**\n",
    "- In a rule-based chatbot, predefined keywords like \"order\" and \"track\" to trigger specific responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09e2dc9f-4e03-4859-a7a9-138f64e0ab77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How may I help you?\n",
      " Where is my order?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your order number to track your order.\n"
     ]
    }
   ],
   "source": [
    "# Define a function for a simple rule-based chatbot\n",
    "def rule_based_chatbot(user_input):\n",
    "    # Check if the user input contain the words \"track\" or \"order\"\n",
    "    if \"track\" in user_input.lower() or \"order\" in user_input.lower():\n",
    "        # Respond with a prompt to provide an order number\n",
    "        return \"Please provide your order number to track your order.\"\n",
    "\n",
    "    elif \"refund\" in user_input.lower():\n",
    "        # Respond with information about the refund policy\n",
    "        return \"For a refund, please visit our refund policy page.\"\n",
    "\n",
    "\n",
    "    # If the input doesn't match any of the predefined rules\n",
    "    else:\n",
    "        # Respond with a message indicating the chatbot doesn't understand the query\n",
    "        return \"I'm sorry, I didn't understand that. Can you try again?\"\n",
    "\n",
    "# Example user input\n",
    "user_query = input(\"How may I help you?\\n\")\n",
    "\n",
    "print(rule_based_chatbot(user_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cc0876-7302-477c-b53e-8e76649c4c4c",
   "metadata": {},
   "source": [
    "#### **2. Retrieval-Based Chatbot Example (Jaccard Similarity):**\n",
    "- In a retrieval-based chatbot, the bot looks for similar sentences in a predefined set of responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e026218-f858-4b0f-aced-2d8e8c25a621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How may I help you? \n",
      " where is my order?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your order number to track your order.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# A predefined set of possible responses for the chatbot stored as a list\n",
    "responses = [\n",
    "    \"Please provide your order number to track your order.\",\n",
    "    \"For a refund, please visit our refund policy page.\",\n",
    "    \"Our customer service is available 24/7.\"\n",
    "]\n",
    "\n",
    "# Load a set of English stopwords (common words that may be removed in text preprocessing)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# define preprocess function to clean and prepare text data\n",
    "def preprocess(text):\n",
    "    # Tokenize the input text into individual words and convert them to lowercase\n",
    "    words = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stopwords (e.g 'the', 'is') and punctuations\n",
    "    words = [word for word in words if word not in stop_words and word not in string.punctuation]\n",
    "\n",
    "    # Return the cleaned list of words\n",
    "    return words\n",
    "\n",
    "\n",
    "# define Function to calculate Jaccard similarity between two sentences\n",
    "def jaccard_similarity(query, sentence):\n",
    "    #Preprocess the query and the sentence\n",
    "    query_set = set(preprocess(query))\n",
    "    sentence_set = set(preprocess(sentence))\n",
    "\n",
    "    # Calculate the intersection and union of the sets and return Jaccard similarity score\n",
    "    return len(query_set.intersection(sentence_set)) / len(query_set.union(sentence_set))\n",
    "\n",
    "\n",
    "# Define function to find the most relevant response based on user input\n",
    "def retrieval_based_chatbot(user_input):\n",
    "    best_response = \"\" # Placeholder for the best matching response\n",
    "    highest_similarity = 0 \n",
    "\n",
    "    # Loop through each predefined response and calculate the Jaccard similarity\n",
    "    for response in responses:\n",
    "        similarity = jaccard_similarity(user_input, response)\n",
    "\n",
    "        # Update the best response if the current response has a higher similarity score\n",
    "        if  similarity > highest_similarity:\n",
    "            highest_similarity = similarity\n",
    "            best_response = response\n",
    "\n",
    "    # Return the best response\n",
    "    return best_response if best_response else \"I'm sorry, I couldn't find a relevant response.\"\n",
    "\n",
    "\n",
    "\n",
    "# User Input\n",
    "user_query = input(\"How may I help you? \\n\")\n",
    "\n",
    "print(retrieval_based_chatbot(user_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2ee191f-b8fa-4d46-b830-8fa4205dd7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ce95476-8baf-46f2-b58f-e1177cbb0b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4df2a288-4785-4152-b6a9-53d0e53dc25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94312d49-0e04-4529-9c9b-2a24ae538146",
   "metadata": {},
   "source": [
    "#### **3. Generative Chabot**\n",
    "\n",
    "In a generative chatbot, the response is generated dynamically using a machine learning model (like GPT). This would involve training a deep learning model.\n",
    "\n",
    "\n",
    "- **How it works**: The generative chatbot creates a new response based on the user input, generating an original sentence that wasn't pre-programmed or retrieved from a predefined list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d4ee8a-5d6d-410f-9cc0-ff0a58632acf",
   "metadata": {},
   "source": [
    "## **D. Common Terms in Natural Language Processing (NLP)**\n",
    "\n",
    "### 1. **Natural Language Processing (NLP)**\n",
    "NLP is a way for computers to  understand, interpret, and respond to human language. With NLP, computers can read, listen, and even reply like humans.\n",
    "\n",
    "\n",
    "### 2. **Tokenization**\n",
    "Tokenization is breaking down a sentence into smaller pieces that a computer can understand\n",
    "\n",
    "\n",
    "### 3. **Lemmatization**\n",
    "Lemmatization is when the computer changes word to their simplest form, called the **Lemma**. For example, the word \"running\" changes to \"run\".\n",
    "\n",
    "\n",
    "\n",
    "### 4. **Stemming**\n",
    "Stemming is when the computer cuts off the ends of words to get the base form, or **stem**. For example, \"Playing\", \"played\", and \"plays\" all become \"play\". This is different from lemmatization because it chops off word endings.\n",
    "Stemming helps computers group words with similar meanings together by choppings off extra endings.\n",
    "\n",
    "\n",
    "\n",
    "### 5. **Stopwords**\n",
    "Stopwords are very common words, like \"the\", \"is\", \"and\", \"in\" that computers often ignore when analyzing a sentence.\n",
    "\n",
    "\n",
    "\n",
    "### 6. **Corpus**\n",
    "A **corpus** is a large collection of written or spoken text that computers use to learn and analyze language. It's like giving  the computer lots of books to read and study from.\n",
    "\n",
    "\n",
    "\n",
    "### 7. **Bag of Words (BOW)**\n",
    "Is a simple way for computers tor epresent text. It works by counting how many times each word appears in a text, without caring about the order of the words.\n",
    "\n",
    "\n",
    "\n",
    "### 8. **TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
    "TF-IDF is a more advanced version of **Bag of Words**. It doesn't just count how often a word appears in a text (like BOW), it also checks how rare or important that word is across many documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c4a521-ab16-430b-8eb2-fa1ba36e1cae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a76b43fa-ba4e-475f-b602-f52a5a39a390",
   "metadata": {},
   "source": [
    "## **E. Workflow for Building a Simple Chatbot using NLTK (Natural Language Toolkit)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a5dd498-fc8a-4e6c-a9f0-1e27f9ab1055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install NLTK and Download all necessary resources \n",
    "# !pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('punkt') \n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt_tab')\n",
    "\n",
    "# Alternatively\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e393ef7b-41cb-4136-9dc1-8e13f96525f6",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a41e910-ea64-43a0-9670-ba4a67447e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the alice_in_wonderland.txt file \n",
    "\n",
    "with open('alice_in_wonderland.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84778b6e-e7de-4c7f-8be9-d2897dab59d2",
   "metadata": {},
   "source": [
    "#### Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f403a490-08f0-40a9-ba05-3edafc25b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "\n",
    "# Initialize stopwords and Lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Use a function for preprocessing each sentence\n",
    "def preprocess(sentence):\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "\n",
    "# Tokenize text into sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "corpus = [preprocess(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a6ff54-65a5-4e4c-84e2-b808c972521d",
   "metadata": {},
   "source": [
    "### Implement Jaccard Similarity for Response MAtching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad46b696-aa1a-48fe-ac20-a985100d59ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Function to calculate Jaccard similarity between two sentences\n",
    "def jaccard_similarity(query, sentence):\n",
    "    #Preprocess the query and the sentence\n",
    "    query_set = set(preprocess(query))\n",
    "    sentence_set = set((sentence))\n",
    "\n",
    "    # Calculate the intersection and union of the sets and return Jaccard similarity score\n",
    "    return len(query_set.intersection(sentence_set)) / len(query_set.union(sentence_set))\n",
    "\n",
    "\n",
    "def get_response(query):\n",
    "    max_similarity = 0\n",
    "    best_response = \"\"\n",
    "    for i, sentence in enumerate(corpus):\n",
    "        similarity = jaccard_similarity(query, sentence)\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            best_response = sentences[i]\n",
    "    return best_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ac647dbf-45ea-4cf2-b6cf-2908e2cff0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What can I help you with? \n",
      " How does Alice enter Wonderland?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poor Alice!\n"
     ]
    }
   ],
   "source": [
    "user_query = input(\"What can I help you with? \\n\")\n",
    "response = get_response(user_query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ac2419-23a9-4b46-bf27-342864e5ae22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54ac72b-451a-46b3-80f8-5fa5550574a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import streamlit as st\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load stopwords and initialize lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess(sentence):\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    words = [word for word in words if word not in stop_words and word not in string.punctuation]\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return words\n",
    "\n",
    "\n",
    "# Load the text file\n",
    "@st.cache_data\n",
    "def load_text():\n",
    "    try:\n",
    "        file_path = \"alice_in_wonderland.txt\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read().replace('\\n', ' ')\n",
    "    except FileNotFoundError:\n",
    "        st.error(\n",
    "            \"Text file not found. Please ensure 'alice_in_wonderland.txt' is in the same directory as this script.\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# Prepare corpus\n",
    "@st.cache_resource\n",
    "def prepare_corpus(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return [preprocess(sentence) for sentence in sentences]\n",
    "\n",
    "\n",
    "# Calculate Jaccard similarity\n",
    "def jaccard_similarity(query, sentence):\n",
    "    query_set = set(query)\n",
    "    sentence_set = set(sentence)\n",
    "    if len(query_set.union(sentence_set)) == 0:\n",
    "        return 0\n",
    "    return len(query_set.intersection(sentence_set)) / len(query_set.union(sentence_set))\n",
    "\n",
    "\n",
    "# Find the most relevant sentence\n",
    "def get_most_relevant_sentence(query, corpus, original_sentences):\n",
    "    query = preprocess(query)\n",
    "    max_similarity = 0\n",
    "    best_sentence = \"I couldn't find a relevant answer.\"\n",
    "    for i, sentence in enumerate(corpus):\n",
    "        similarity = jaccard_similarity(query, sentence)\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            best_sentence = original_sentences[i]\n",
    "    return best_sentence\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    st.title(\"Wonderland's Novice Chatbot\")\n",
    "    st.write(\"Hello! Ask me anything related to Alice in Wonderland!\")\n",
    "\n",
    "    with st.expander(\"Click me for suggestions\"):\n",
    "        st.write(\"\"\"\n",
    "        1. Who does Alice meet first in Wonderland?\n",
    "        2. What is the Cheshire Cat's famous line?\n",
    "        3. How does Alice enter Wonderland?\n",
    "        4. What is the Queen of Hearts known for?\n",
    "        5. Why did Alice follow the White Rabbit?\n",
    "        6. What was Alice's reaction to the Mad Hatter's tea party?\n",
    "        7. What advice does the Caterpillar give Alice?\n",
    "        8. What is the significance of the bottle labeled 'Drink Me'?\n",
    "        9. How does the story of Alice in Wonderland end?\n",
    "        10. What game does the Queen of Hearts play with Alice?\n",
    "        \"\"\")\n",
    "\n",
    "    text = load_text()\n",
    "    if text:\n",
    "        corpus = prepare_corpus(text)\n",
    "        original_sentences = sent_tokenize(text)\n",
    "\n",
    "        user_input = st.text_input(\"Enter your question:\")\n",
    "\n",
    "        if st.button(\"Submit\"):\n",
    "            if user_input.strip():\n",
    "                response = get_most_relevant_sentence(user_input, corpus, original_sentences)\n",
    "                st.write(f\"Chatbot: {response}\")\n",
    "            else:\n",
    "                st.write(\"Please enter a question.\")\n",
    "\n",
    "\n",
    "# Run the app\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888ab6b6-807d-4da8-8936-78ce4b6710a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
